{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "filled-article",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is NoteBook File that i created for Get Prediction from each image'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This is NoteBook File that i created for Get Prediction from each image\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flush-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing Dependacies\n",
    "# !pip install -U layoutparser\n",
    "# !pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.4#egg=detectron2'\n",
    "# !pip install layoutparser[ocr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "structured-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unique-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Title from the Image and return image with those Coordinates\n",
    "\n",
    "def doc_title_segement(file_path):\n",
    "    \n",
    "    \n",
    "    # Configurations\n",
    "    # HJDataset\t{1:\"Page Frame\", 2:\"Row\", 3:\"Title Region\", 4:\"Text Region\", 5:\"Title\", 6:\"Subtitle\", 7:\"Other\"}\n",
    "    # PubLayNet\t{0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}\n",
    "    # PrimaLayout\t{1:\"TextRegion\", 2:\"ImageRegion\", 3:\"TableRegion\", 4:\"MathsRegion\", 5:\"SeparatorRegion\", 6:\"OtherRegion\"}\n",
    "    # NewspaperNavigator\t{0: \"Photograph\", 1: \"Illustration\", 2: \"Map\", 3: \"Comics/Cartoon\", 4: \"Editorial Cartoon\", 5: \"Headline\", 6: \"Advertisement\"}\n",
    "    # TableBank\t{0: \"Table\"}\n",
    "    \n",
    "    # Loading Layout Parser Model\n",
    "    model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.65],\n",
    "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n",
    "    \n",
    "    # This will store crop title from images \n",
    "    # Give Benefit at batch processing in yolo\n",
    "    crop_image_title = []\n",
    "    \n",
    "    for file_path in os.scandir(file_path):\n",
    "        if file_path.is_file():\n",
    "            \n",
    "            #Read image and convert into RGB\n",
    "            image = cv2.imread(file_path.path)[..., ::-1]\n",
    "            \n",
    "            layout = model.detect(image)\n",
    "            \n",
    "            \n",
    "            # Extract Title Region Only\n",
    "            text_blocks = lp.Layout([b for b in layout if b.type==\"Title\"]) \n",
    "            \n",
    "            \n",
    "            figure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])\n",
    "            \n",
    "            \n",
    "            text_blocks = lp.Layout([b for b in text_blocks \\\n",
    "                   if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "            \n",
    "            \n",
    "            \n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
    "\n",
    "            left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
    "            left_blocks.sort(key = lambda b:b.coordinates[1])\n",
    "\n",
    "            right_blocks = [b for b in text_blocks if b not in left_blocks]\n",
    "            right_blocks.sort(key = lambda b:b.coordinates[1])\n",
    "\n",
    "            # And finally combine the two list and add the index\n",
    "            # according to the order\n",
    "            text_blocks = lp.Layout([b.set(id = idx) \n",
    "                                     for idx, b in \n",
    "                                     enumerate(left_blocks + right_blocks)])\n",
    "            \n",
    "            \n",
    "            #\n",
    "                    \n",
    "            \n",
    "            \n",
    "            for dic in text_blocks.to_dict()[\"blocks\"]:\n",
    "                \n",
    "                # Extract x1,y1,x_2,y_2 From dictionary\n",
    "                x1,y1,x2,y2 = dic[\"x_1\"],dic[\"y_1\"],dic[\"x_2\"],dic[\"y_2\"]\n",
    "                \n",
    "                print(x1,y1,x2,y2)\n",
    "                \n",
    "                \n",
    "                \n",
    "                crop_image_title.append(image[int(y1-3):int(y2+3),int(x1-3):int(x2+3)])\n",
    "                \n",
    "                \n",
    "        return crop_image_title\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "military-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Array Of images and predict each of image using YOlo v5\n",
    "def yolo_model_heading_detection(crop_title_array):\n",
    "    \n",
    "    \n",
    "    # Best Weight Model Path\n",
    "    model_path = \"/home/nipun/Documents/NIpun/Git_Repos/Document-Segmentation/Doc_Segemnt/best.pt\"\n",
    "    \n",
    "    \n",
    "    # In order to load model from local need to add  source='local'\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path,force_reload=True)\n",
    "    \n",
    "    \n",
    "    # Add Test time augmentation for better prediction\n",
    "    results = model(crop_images, size=640,augment=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # If we need we can return panda dataframe\n",
    "    # results.pandas().xyxy[0]\n",
    "        \n",
    "    \n",
    "    # To Json\n",
    "    # results.pandas().xyxy[0].to_json(orient=\"records\")\n",
    "    \n",
    "    # To save Results\n",
    "    results.save()\n",
    "    \n",
    "    \n",
    "    return results.print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accompanied-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This will extract content from image and will return dictionary\n",
    "def doc_text_segement(file_path):\n",
    "    \n",
    "    \n",
    "    # Configurations\n",
    "    # HJDataset\t{1:\"Page Frame\", 2:\"Row\", 3:\"Title Region\", 4:\"Text Region\", 5:\"Title\", 6:\"Subtitle\", 7:\"Other\"}\n",
    "    # PubLayNet\t{0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}\n",
    "    # PrimaLayout\t{1:\"TextRegion\", 2:\"ImageRegion\", 3:\"TableRegion\", 4:\"MathsRegion\", 5:\"SeparatorRegion\", 6:\"OtherRegion\"}\n",
    "    # NewspaperNavigator\t{0: \"Photograph\", 1: \"Illustration\", 2: \"Map\", 3: \"Comics/Cartoon\", 4: \"Editorial Cartoon\", 5: \"Headline\", 6: \"Advertisement\"}\n",
    "    # TableBank\t{0: \"Table\"}\n",
    "    \n",
    "    # Loading Layout Parser Model\n",
    "    model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.65],\n",
    "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n",
    "    \n",
    "    \n",
    "    text_array = []\n",
    "    \n",
    "    for file_path in os.scandir(file_path):\n",
    "        if file_path.is_file():\n",
    "            \n",
    "            #Read image and convert into RGB\n",
    "            image = cv2.imread(file_path.path)[..., ::-1]\n",
    "            \n",
    "            layout = model.detect(image)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Extract Text Region Only\n",
    "            text_blocks = lp.Layout([b for b in layout if b.type==\"Text\"]) \n",
    "            \n",
    "            \n",
    "            figure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])\n",
    "            \n",
    "            \n",
    "            text_blocks = lp.Layout([b for b in text_blocks \\\n",
    "                   if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "            \n",
    "            \n",
    "            \n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
    "\n",
    "            left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
    "            left_blocks.sort(key = lambda b:b.coordinates[1])\n",
    "\n",
    "            right_blocks = [b for b in text_blocks if b not in left_blocks]\n",
    "            right_blocks.sort(key = lambda b:b.coordinates[1])\n",
    "\n",
    "            # And finally combine the two list and add the index\n",
    "            # according to the order\n",
    "            text_blocks = lp.Layout([b.set(id = idx) \n",
    "                                     for idx, b in \n",
    "                                     enumerate(left_blocks + right_blocks)])\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Append Coordinates into Text Array\n",
    "            text_array.append(text_blocks.to_dict()[\"blocks\"])\n",
    "                    \n",
    "            \n",
    "            \n",
    "\n",
    "                \n",
    "                \n",
    "        return text_array\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "future-ribbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from https://www.dropbox.com/s/57zjbwv6gh3srry/model_final.pth?dl=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'x_1': 83.01058959960938,\n",
       "   'y_1': 534.6427001953125,\n",
       "   'x_2': 171.05123901367188,\n",
       "   'y_2': 551.2643432617188,\n",
       "   'block_type': 'rectangle',\n",
       "   'id': 0,\n",
       "   'type': 'Text',\n",
       "   'score': 0.8441960215568542},\n",
       "  {'x_1': 54.133460998535156,\n",
       "   'y_1': 18.816341400146484,\n",
       "   'x_2': 908.9793090820312,\n",
       "   'y_2': 92.06700134277344,\n",
       "   'block_type': 'rectangle',\n",
       "   'id': 1,\n",
       "   'type': 'Text',\n",
       "   'score': 0.8043978214263916},\n",
       "  {'x_1': 41.26341247558594,\n",
       "   'y_1': 164.6133575439453,\n",
       "   'x_2': 886.8906860351562,\n",
       "   'y_2': 235.06654357910156,\n",
       "   'block_type': 'rectangle',\n",
       "   'id': 2,\n",
       "   'type': 'Text',\n",
       "   'score': 0.7681299448013306},\n",
       "  {'x_1': 32.65528106689453,\n",
       "   'y_1': 113.68264770507812,\n",
       "   'x_2': 880.6586303710938,\n",
       "   'y_2': 154.24501037597656,\n",
       "   'block_type': 'rectangle',\n",
       "   'id': 3,\n",
       "   'type': 'Text',\n",
       "   'score': 0.746097981929779},\n",
       "  {'x_1': 75.462158203125,\n",
       "   'y_1': 247.4720001220703,\n",
       "   'x_2': 845.6636962890625,\n",
       "   'y_2': 265.3075866699219,\n",
       "   'block_type': 'rectangle',\n",
       "   'id': 4,\n",
       "   'type': 'Text',\n",
       "   'score': 0.7373899221420288},\n",
       "  {'x_1': 60.471435546875,\n",
       "   'y_1': 277.17724609375,\n",
       "   'x_2': 897.1239013671875,\n",
       "   'y_2': 321.9419860839844,\n",
       "   'block_type': 'rectangle',\n",
       "   'id': 5,\n",
       "   'type': 'Text',\n",
       "   'score': 0.7286473512649536},\n",
       "  {'x_1': 24.7130126953125,\n",
       "   'y_1': 339.61822509765625,\n",
       "   'x_2': 895.154541015625,\n",
       "   'y_2': 420.3045654296875,\n",
       "   'block_type': 'rectangle',\n",
       "   'id': 6,\n",
       "   'type': 'Text',\n",
       "   'score': 0.7107455730438232},\n",
       "  {'x_1': 612.6408081054688,\n",
       "   'y_1': 535.4290771484375,\n",
       "   'x_2': 714.881591796875,\n",
       "   'y_2': 551.6444091796875,\n",
       "   'block_type': 'rectangle',\n",
       "   'id': 7,\n",
       "   'type': 'Text',\n",
       "   'score': 0.7106644511222839}]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_text_segement(\"./Images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "integral-confidentiality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from https://www.dropbox.com/s/57zjbwv6gh3srry/model_final.pth?dl=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.91815185546875 437.9176940917969 503.8951416015625 455.52581787109375\n",
      "640.8719482421875 577.3986206054688 871.2575073242188 675.0\n"
     ]
    }
   ],
   "source": [
    "crop_images=doc_title_segement(\"./Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "confirmed-macro",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /home/nipun/.cache/torch/hub/master.zip\n",
      "YOLOv5 🚀 2021-8-9 torch 1.7.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n",
      "Adding AutoShape... \n",
      "Saved 2 images to 'runs/detect/exp'\n",
      "image 1/2: 24x427 2 h3s\n",
      "image 2/2: 101x237 (no detections)\n",
      "Speed: 1.4ms pre-process, 141.0ms inference, 0.5ms NMS per image at shape (2, 3, 288, 640)\n"
     ]
    }
   ],
   "source": [
    "yolo_model_heading_detection(crop_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sudden-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_image_segement(file_path):\n",
    "    \n",
    "    \n",
    "    #ConfigurationS\n",
    "    # HJDataset\t{1:\"Page Frame\", 2:\"Row\", 3:\"Title Region\", 4:\"Text Region\", 5:\"Title\", 6:\"Subtitle\", 7:\"Other\"}\n",
    "    # PubLayNet\t{0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}\n",
    "    # PrimaLayout\t{1:\"TextRegion\", 2:\"ImageRegion\", 3:\"TableRegion\", 4:\"MathsRegion\", 5:\"SeparatorRegion\", 6:\"OtherRegion\"}\n",
    "    # NewspaperNavigator\t{0: \"Photograph\", 1: \"Illustration\", 2: \"Map\", 3: \"Comics/Cartoon\", 4: \"Editorial Cartoon\", 5: \"Headline\", 6: \"Advertisement\"}\n",
    "    # TableBank\t{0: \"Table\"}\n",
    "    \n",
    "    # Loading Layout Parser Model\n",
    "    model = lp.Detectron2LayoutModel('lp://PrimaLayout/mask_rcnn_R_50_FPN_3x/config',extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.65],\n",
    "                                 label_map={1:\"TextRegion\", 2:\"ImageRegion\", 3:\"TableRegion\", 4:\"MathsRegion\", 5:\"SeparatorRegion\", 6:\"OtherRegion\"})\n",
    "    \n",
    "    # This will returrn location of the images\n",
    "    image_location_array = []\n",
    "    \n",
    "    for file_path in os.scandir(file_path):\n",
    "        \n",
    "        \n",
    "        # Read Images as RGB\n",
    "        image = cv2.imread(file_path.path)[..., ::-1]\n",
    "        \n",
    "        layout = model.detect(image) \n",
    "        \n",
    "        \n",
    "        image_blocks = lp.Layout([b for b in layout if b.type==\"ImageRegion\"])\n",
    "        \n",
    "        \n",
    "        image_location_array.append(image_blocks.to_dict()[\"blocks\"])\n",
    "        \n",
    "        \n",
    "    return image_location_array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "retired-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from https://www.dropbox.com/s/h7th27jfv19rxiy/model_final.pth?dl=1\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mpixel_mean\u001b[0m\n",
      "  \u001b[35mpixel_std\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_image_segement(\"./Images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-norwegian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron",
   "language": "python",
   "name": "detectron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
